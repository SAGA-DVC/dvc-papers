# Temporal Action Proposal Generation with Transformers
Temporal Action Proposal Generation: 
> Predict segments(proposals) in the video that correspond to an action. The segment includes the start and end time along with its confidence score.

[The paper.](https://arxiv.org/abs/2105.12043)

Recent paper - 2021 May. Achieves SOTA on both datasets used. 

## Features
- **TAPG Transformer** which consists of **Boundary Transformer** and **Proposal Transformer**.
- **Boundary Transformer** captures long-term temporal dependencies to predict precise boundary information.
- **Proposal Transformer** learns the rich inter-proposal relationships for reliable confidence evaluation.
- **Sparse Sampling Mechanism** to generate the sparse proposal sequence instead of the densely distributed proposals which will bring imbalanced data distribution between positive/negative proposals and more computational burdens.
- **Fuzzy Matching** is used to determine the proposals using the outputs of the Boundary Transformer and Proposal Transformer

## Datasets used
- ActivityNet-1.3
- THUMOS14 

## Preprocessing
1. Input: 
	- untrimmed video U can be denoted as a frame sequence U = {u<sub>t</sub>}<sub>t=1</sub> <sup>l<sub>v</sub></sup> with l<sub>v</sub> frames.
    - the temporal annotation set consists of n ground truth action instances each with a start time and end time.
2. Feature extraction:
    - The video U is divided into T = l<sub>v</sub>/sigma timesteps to decrease computation. 
    - These uniformly sampled segments are then fed into a feature extractor. 
    - The output consists of a feature sequence F = {f<sub>i</sub>}<sub>i=1</sub> <sup>T</sup> which is fed into the network
    - The feature extractor used here is [Two-Stream Convolutional Networks for Action Recognition in Videos](https://papers.nips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf) but we could use other ones such as ResNet or C3D too.

## Boundary Transformer
1. Encoder gets as input F ∈ R<sup>TxC</sup> and it outputs temporal features of the same dimension
2. Decoder gets the same input as the encoder but consists of cross attention with the output of the encoder
3. A feed forward NN(Boundary Head) gets as input the decoder's output. The sigmoid function is used at the last layer, consisting of 2 nodes(start time and end time). 
4. Thus, output is of dimension Tx2

## Proposal Transformer
1. Sparse Sampling mechanism is used first
    1. Fibonacci sequence is used to determine the group sizes for the sliding window W = {w<sub>i</sub>}<sub>i=1</sub> <sup>D</sup>
    2. step size for each window is calulated as :
    ![](assets/step_size.png) 
    where γ is used to constraint the step size increase with the increase in the window size.
    3. Weight matrix of shape NxT (N is the number of points sampled in a temporal region??(window??)) is multiplied with feature matrix of shape TxC to give CxN shaped proposal features. 
    4. The above step is done for L proposals, thus getting the proposal matrix of shape LxS (S = CxN flattended)
    5. LxS proposal features are fed into a transformer.
    6. The output of the decoder is given to a feed forward NN with sigmoid at the last layer. The last layer outputs 2 confidence scores
        1. C<sub>c</sub> - used for binary classification loss
        2. C<sub>r</sub> - used for regression loss

## Fuzzy Matching
1. Boundary transformer generates a set of proposals P consisting of start time and end time {t<sub>s</sub>, t<sub>e</sub>}<sub>p</sub>
2. Fuzzy matching computes tIoU (temporal Intersection over Union) between proposal p and the sparse proposal generated by proposal transformer to select matching proposal {t<sup>m</sup><sub>s</sub>, t<sup>m</sup><sub>e</sub>}<sub>p<sub>m</sub></sub> 
3. proposals are refined using :
![](assets/proposal_refine.png)
4. Soft-NMS algorithm for Post-processing to remove the proposals which highly overlap with each other.


## Training
### Loss
1. L = L<sub>b</sub> + L<sub>p</sub>
2. L<sub>b</sub> is the loss for boundary transformer
    1. L<sub>b</sub> is split into separate losses for start and end time - L<sub>b</sub> = L<sub>s</sub> + L<sub>e</sub>
    2. L<sub>s</sub> and L<sub>e</sub> - binary logistic loss(log loss)
2. L<sub>p</sub> is the loss for proposal transformer
    1. L<sub>p</sub> is split into separate losses for classification confidence and regression confidence - L<sub>p</sub> = Lc + Lr
    2. L<sub>c</sub> - binary logistic loss(log loss)
    3. L<sub>r</sub> - smooth L1 loss

![](assets/model.png)

