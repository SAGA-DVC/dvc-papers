# dvc-papers
A repository to track research papers and notes

## Table of Papers
| # | Name | Year | Paper Link | Description |
| --- | --- | --- | --- | --- |
| 1 | [A Better Use of Audio-Visual Cues - Dense Video Captioning with Bi-modal Transformer](A_Better_Use_of_Audio-Visual_Cues_Dense_Video_Captioning_with_Bi-modal_Transformer/README.md) | 2020 | [:link:](https://arxiv.org/abs/2005.08271) | Bi-modal transformer, multi-head proposal generator, attentive fusion of input sequences |
| 2 | [Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning](Bidirectional_Attentive_Fusion_with_Context_Gating/README.md) | 2018 | [:link:](https://arxiv.org/abs/1804.00100) | Bi-SST for past and future context, temporal dynamic attention for fusing visual features and context vectors as input to decoder, context gating, joint ranking |
| 3 | [Temporally Sensitive Pretraining of Video Encoders for Localisation Tasks](Temporally_Sensitive_Pretraining_of_Video_Encoders_for_Localization_Tasks/README.md) | 2021 | [:link:](https://arxiv.org/abs/2011.11479) | Extracting temporally sensitive features from video, instead of temporally-insensitive features like TAC pre-trained encoders do (which most papers use) |
| 4 | [Temporal Action Proposal Generation with Transformers](TAPG/Temporal_Action_Proposal_Generation_with_Transformers/README.md) | 2021 | [:link:](https://arxiv.org/abs/2105.12043) | Novel temporal action proposal generation framework with transformers, which consists a Boundary Transformer and a Proposal Transformer used to generate temporal action proposals |
| 5 | [Jointly Localizing and Describing Events for Dense Video Captioning](Jointly_Localizing_and_Describing_Events_for_Dense_Video_Captioning/README.md) | 2018 | [:link:](https://arxiv.org/abs/1804.08274) | Event proposal and sentence generation in end to end manner. Introduces descriptiveness score and eventness score |
| 6 | [Listen to Look](Listen_to_Look/README.md) | 2020 | [:link:](https://arxiv.org/abs/1912.04487) | Classifying long untrimmed videos using visual and audio modalities |
| 7 | [iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering](iPerceive_Applying_Common-Sense_Reasoning_to_Multi-Modal_Dense_Video_Captioning_and_Video_Question_Answering/README.md) | 2020 | [:link:](https://arxiv.org/abs/2011.07735) | DVC with main focus on reducing cognitive error and attending to objects |
| 8 | [Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](I3D_Action_Recognition_Kinetics_Dataset/README.md) | 2018 | [:link:](https://arxiv.org/abs/1705.07750) | I3D Model for extraction of visual features |
| 9 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - ViT](ViT/README.md) | 2020/2021 | [:link:](https://arxiv.org/abs/2010.11929) | Vision Transformer |
| 10 | [Anticipative Video Transformer - AVT](AVT/README.md) | 2021 | [:link:](https://arxiv.org/abs/2106.02036) | Action Anticipation task |
| 11 | [Towards Automatic Learning of Procedures from Web Instructional Videos](Towards_Automatic_Learning_of_Procedures_from_Web_Instructional_Videos/README.md) | 2017 | [:link:](https://arxiv.org/abs/1703.09788) | Generating proposals for procedural video |
| 12 | [Dense-Captioning Events in Videos](Dense_Captioning_Events_in_Videos/README.md) | 2017 | [:link:](https://arxiv.org/abs/1705.00754) | Introduced DVC and ActivityNet Captions dataset. Architecture inspired by DAPs. |
| 13 | [End-to-end Dense Video Captioning with Parallel Decoding](PDVC_End_to_End_DVC_with_Parallel_Decoding/README.md) | 2021 | [:link:](https://arxiv.org/abs/2108.07781) | Localization and caption generation in parallel; end-to-end model; event number estimation; deformable transformer, MSDAtt, Deformable Soft Attention |
